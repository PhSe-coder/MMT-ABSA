{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tag_utils import Annotate, Annotation_mentions\n",
    "txt = \"I have recently converted back to a mac and I could n't be happier !\"\n",
    "obj = Annotation_mentions(txt)\n",
    "for i in obj.keys():\n",
    "    print(i + \"  \" + obj[i])\n",
    "print(\"=\" * 30)\n",
    "obj = Annotate(txt, theta=0.05)\n",
    "for k in obj:\n",
    "    print(obj[k][2], \" ---> \", obj[k][1], obj[k][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "\n",
    "def process_ann(sentence: str):\n",
    "    result = {}\n",
    "    for score, mention, entity_title, entity_id, uri in Annotate(sentence, theta=0.05).values():\n",
    "        if entity_title in result:\n",
    "            if len(result[entity_title]) < len(mention):\n",
    "                result[entity_title] = mention\n",
    "        else:\n",
    "            result[entity_title] = mention\n",
    "    return result.values()\n",
    "save_dir = \"./entities\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for domain in (\"rest\", \"laptop\", \"service\", \"device\"):\n",
    "    entities = []\n",
    "    for file in glob(f\"../data/{domain}.*.txt\"):\n",
    "        counts = sum(1 for _ in open(file))\n",
    "        sentences = [line.split(\"***\")[0] for line in open(file).read().splitlines()]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=100) as t:\n",
    "            for future in tqdm(as_completed(\n",
    "                [t.submit(process_ann, sentence) for sentence in sentences]),\n",
    "                               total=counts,\n",
    "                               desc=file):\n",
    "                entities.extend(future.result())\n",
    "    with open(os.path.join(save_dir, f\"{domain}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(entities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidata.client import Client\n",
    "import urllib\n",
    "proxies = {\"http\": \"http://127.0.0.1:7890\", \"https\": \"http://127.0.0.1:7890\"}\n",
    "# 设置代理\n",
    "handler = urllib.request.ProxyHandler(proxies)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "client = Client(opener=opener)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.entity import WikidataItem, WikidataLexeme, WikidataProperty\n",
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "\n",
    "# create an item representing \"Douglas Adams\"\n",
    "Q_DOUGLAS_ADAMS = \"Q42\"\n",
    "q42_dict = get_entity_dict_from_api(Q_DOUGLAS_ADAMS)\n",
    "q42 = WikidataItem(q42_dict)\n",
    "\n",
    "# create a property representing \"subclass of\"\n",
    "P_SUBCLASS_OF = \"P279\"\n",
    "p279_dict = get_entity_dict_from_api(P_SUBCLASS_OF)\n",
    "p279 = WikidataProperty(p279_dict)\n",
    "\n",
    "# create a lexeme representing \"bank\"\n",
    "L_BANK = \"L3354\"\n",
    "l3354_dict = get_entity_dict_from_api(L_BANK)\n",
    "l3354 = WikidataLexeme(l3354_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.sparql import return_sparql_query_results\n",
    "\n",
    "# send any sparql query to the wikidata query service and get full result back\n",
    "# here we use an example that counts the number of humans\n",
    "sparql_query = \"\"\"\n",
    "SELECT (COUNT(?item) AS ?count)\n",
    "WHERE {\n",
    "        ?item wdt:P31/wdt:P279* wd:Q5 .\n",
    "}\n",
    "\"\"\"\n",
    "return_sparql_query_results(sparql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.sparql import get_subclasses_of_item\n",
    "# use convenience function to get subclasses of an item as a list of item ids\n",
    "Q_RIVER = \"Q214276\"\n",
    "get_subclasses_of_item(Q_RIVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "hyper = lambda s: s.hypernyms()\n",
    "for domain in (\"laptop\",):\n",
    "    for synset in wn.synsets(domain, pos=wn.NOUN):\n",
    "        print(synset.hyponyms())\n",
    "        print(list(synset.closure(hyper)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "for ann in tagme.annotate(\"MacBook Pro\").get_annotations(0.1):\n",
    "    json = requests.get(\n",
    "    \"https://en.wikipedia.org/w/api.php?action=query&format=json&prop=pageprops&titles=MacBook Pro\"\n",
    "    ).json()\n",
    "    wikidata_id = json['query']['pages'][f'{ann.entity_id}']['pageprops']['wikibase_item']\n",
    "    print(ann.uri(), ann.entity_id, ann.entity_title, wikidata_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from eval import absa_evaluate, evaluate\n",
    "from tag_utils import ot2bio_absa\n",
    "text = [\n",
    "    line.split(\"***\")[0]\n",
    "    for line in open(\"/root/graduation/processed/dp_tmp/laptop.train.txt\").read().splitlines()\n",
    "]\n",
    "gold_Y = [\n",
    "    ot2bio_absa(line.split(\"***\")[1].split())\n",
    "    for line in open(\"/root/graduation/processed/dp_tmp/laptop.train.txt\").read().splitlines()\n",
    "]\n",
    "\n",
    "pred_Y = [\n",
    "    ot2bio_absa(line.split(\"***\")[-1].split())\n",
    "    for line in open(\"/root/graduation/processed/dp_tmp/laptop.train.txt\").read().splitlines()\n",
    "]\n",
    "pred_Y_ = []\n",
    "gold_Y_ = []\n",
    "for idx, (pred, gold) in enumerate(zip(pred_Y, gold_Y)):\n",
    "    sentence = text[idx]\n",
    "    # res = Annotate(sentence, theta=0.05)\n",
    "    if not all(item == 'O' for item in pred):\n",
    "        pred_Y_.append(pred)\n",
    "        gold_Y_.append(gold)\n",
    "absa_evaluate(pred_Y_, gold_Y_), evaluate(pred_Y_, gold_Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model('/root/autodl-tmp/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "with open(\"./entities/laptop.pkl\", \"rb\") as f:\n",
    "    entities: List[str] = pickle.load(f)\n",
    "# remove stopwords\n",
    "sets = stopwords.words('english')\n",
    "# lemmatization\n",
    "entities = [wnl.lemmatize(word, 'n').lower() for word in entities if word not in sets]\n",
    "counters = Counter(entities)\n",
    "sorted_entities: List[Tuple[str, int]] = sorted(filter(lambda item: item[1] >= 5, counters.items()),\n",
    "                                                key=lambda item: item[1],\n",
    "                                                reverse=True)\n",
    "\n",
    "vec_dict = {}\n",
    "for entity in sorted_entities:\n",
    "    e = entity[0]\n",
    "    vec_dict[e] = model.get_word_vector(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "getter = itemgetter(*[entity for entity, _ in sorted_entities[:10]])\n",
    "mean_vec = np.average(getter(vec_dict), axis=0, weights=[count for _, count in sorted_entities[:10]])\n",
    "res = np.array([cosine_similarity(mean_vec.reshape(1, -1), vec_dict[k].reshape(1, -1)) for k in vec_dict]).squeeze()\n",
    "topk = torch.topk(torch.from_numpy(res), 100).indices.tolist()\n",
    "itemgetter(*topk)(sorted_entities), sorted_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "max_score, clusters = 0, 0\n",
    "\n",
    "X = np.array(list(vec_dict.values()))\n",
    "for i in range(2, 100):\n",
    "    kmeans = KMeans(n_clusters=i).fit(X, sample_weight=[entity[1] for entity in sorted_entities[::-1]])\n",
    "    score = calinski_harabasz_score(X, kmeans.labels_)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        clusters = i\n",
    "max_score, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=clusters).fit(X, sample_weight=[entity[1] for entity in sorted_entities[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(list(vec_dict.keys()))[kmeans.labels_ == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(\n",
    "    np.array(model.get_word_vector(\"macbook\")).reshape(1, -1),\n",
    "    np.array(model.get_word_vector(\"macbook_pro\")).reshape(1, -1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "514bf37689fab66dd9e64044d554682626457c62a79ef980ffeb215c6b270f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
