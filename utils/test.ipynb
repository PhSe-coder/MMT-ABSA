{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'laptop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Extracting eitities in laptop domain: 100%|██████████| 3845/3845 [00:42<00:00, 91.38it/s] \n",
      "100%|██████████| 3685/3685 [00:00<00:00, 38780.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from glob import glob\n",
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import fasttext\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tag_utils import Annotate\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "model = fasttext.load_model('../cc.en.300.bin')\n",
    "def process_ann(idx: int, sentence: str):\n",
    "    result = {}\n",
    "    for score, mention, entity_title, entity_id, uri in Annotate(sentence, theta=0.05).values():\n",
    "        if entity_title in result:\n",
    "            if len(result[entity_title]) < len(mention):\n",
    "                result[entity_title] = mention\n",
    "        else:\n",
    "            result[entity_title] = mention\n",
    "    return idx, result\n",
    "\n",
    "\n",
    "\n",
    "entities_tuple_list = []\n",
    "lines = []\n",
    "# get all sentences in a domain\n",
    "for file in glob(os.path.join(\"../data\", f\"{domain}.*.txt\")):\n",
    "    lines.extend([line.split(\"***\") for line in open(file).read().splitlines()])\n",
    "with ThreadPoolExecutor(max_workers=256) as t:\n",
    "    for future in tqdm(as_completed(\n",
    "        [t.submit(process_ann, idx, line[0]) for idx, line in enumerate(lines)]),\n",
    "                       total=len(lines),\n",
    "                       desc=f\"Extracting eitities in {domain} domain\"):\n",
    "        entities_tuple_list.append(future.result())\n",
    "# entities_tuple_list = sorted(entities_tuple_list, key=lambda item: item[0])\n",
    "entities_dict_list = [entities_dict for _, entities_dict in entities_tuple_list]\n",
    "# remove stopwords\n",
    "sets = stopwords.words('english')\n",
    "entities = [word for entities_dict in entities_dict_list for word in entities_dict.values() if word not in sets]\n",
    "counters = Counter(entities)\n",
    "sorted_entities: List[Tuple[str, int]] = sorted(filter(lambda item: item[1] > 0, counters.items()),\n",
    "                                                key=lambda item: item[1],\n",
    "                                                reverse=True)\n",
    "vec_dict = {}\n",
    "for entity in tqdm(sorted_entities, total=len(counters)):\n",
    "    e = entity[0]\n",
    "    vec_dict[e] = model.get_word_vector(e)\n",
    "getter = itemgetter(*[entity for entity, _ in sorted_entities[:10]])\n",
    "mean_vec = np.average(getter(vec_dict),\n",
    "                      axis=0,\n",
    "                      weights=[count for _, count in sorted_entities[:10]])\n",
    "res = np.array([\n",
    "        cosine_similarity(mean_vec.reshape(1, -1), vec_dict[k].reshape(1, -1)) for k in vec_dict\n",
    "    ]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1275"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects = []\n",
    "for line in lines:\n",
    "    text, labels = line\n",
    "    tokens = text.split()\n",
    "    labels = labels.split()\n",
    "    aspect = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label != 'O':\n",
    "            aspect.append(tokens[idx])\n",
    "            continue\n",
    "        if aspect:\n",
    "            aspects.append(' '.join(aspect))\n",
    "            aspect.clear()\n",
    "len(set(aspects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3685"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37176470588235294"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = torch.topk(torch.from_numpy(res), int(0.6 * res.shape[0])).indices.tolist()\n",
    "k = len([sim for sim in itemgetter(*topk)(res) if sim > 0.1])\n",
    "topk = torch.topk(torch.from_numpy(res), k).indices.tolist()\n",
    "len(set(aspects).intersection(set([item[0] for item in itemgetter(*topk)(sorted_entities)]))) / len(\n",
    "    set(aspects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13363008265910653,\n",
       " 0.1336102307362878,\n",
       " 0.13357236294255548,\n",
       " 0.13354637634459182,\n",
       " 0.13354420182916996,\n",
       " 0.13353221020005662,\n",
       " 0.1335151878377501,\n",
       " 0.13340458987022863,\n",
       " 0.13337646690644944,\n",
       " 0.13328786582825397,\n",
       " 0.13316617281089727,\n",
       " 0.1330690857457708,\n",
       " 0.13300306251251612,\n",
       " 0.13296010141207815,\n",
       " 0.13295992325112432,\n",
       " 0.13291151311877294,\n",
       " 0.13288638409244163,\n",
       " 0.13288321483997764,\n",
       " 0.13271120979978165,\n",
       " 0.1326476574912433,\n",
       " 0.1325895697942457,\n",
       " 0.13253645800368508,\n",
       " 0.13249097367669269,\n",
       " 0.1324701927441511,\n",
       " 0.13243493946764562,\n",
       " 0.1323926639258604,\n",
       " 0.13235951970584223,\n",
       " 0.13212959008538655,\n",
       " 0.132072589003778,\n",
       " 0.13206533705279075,\n",
       " 0.132023038897007,\n",
       " 0.1319693406335541,\n",
       " 0.13192982158114153,\n",
       " 0.13188123941858815,\n",
       " 0.13187393095376168,\n",
       " 0.13186263672039683,\n",
       " 0.13182613452411665,\n",
       " 0.13181609581451298,\n",
       " 0.13170296035181173,\n",
       " 0.1316353726835832,\n",
       " 0.13160814433666035,\n",
       " 0.13160472829731387,\n",
       " 0.13152143551994322,\n",
       " 0.1314968746247112,\n",
       " 0.1313857076582732,\n",
       " 0.13127572653388025,\n",
       " 0.1312188256513346,\n",
       " 0.13115844503900845,\n",
       " 0.1311421142912253,\n",
       " 0.13101538325736553,\n",
       " 0.13099386885120393,\n",
       " 0.1309478239635507,\n",
       " 0.13092368570429302,\n",
       " 0.13090335758278354,\n",
       " 0.13089027590686583,\n",
       " 0.13084419311920886,\n",
       " 0.13076797296795115,\n",
       " 0.13071118167243778,\n",
       " 0.13070415609931904,\n",
       " 0.13068481924407144,\n",
       " 0.13058659880855866,\n",
       " 0.1305195191574949,\n",
       " 0.13044509531299775,\n",
       " 0.1304157233236123,\n",
       " 0.1303746296120335,\n",
       " 0.1303639095387929,\n",
       " 0.13031433919516539,\n",
       " 0.1302857125369399,\n",
       " 0.13021783269818915,\n",
       " 0.13011171006062552,\n",
       " 0.13004041100697583,\n",
       " 0.13000519092499646,\n",
       " 0.129988573676209,\n",
       " 0.12989893617126658,\n",
       " 0.12987897009529117,\n",
       " 0.1298355820061693,\n",
       " 0.1298183208794303,\n",
       " 0.12975906470835696,\n",
       " 0.1297317934720873,\n",
       " 0.12962281643792403,\n",
       " 0.12960847288247154,\n",
       " 0.12952218018934916,\n",
       " 0.12951677521016422,\n",
       " 0.12949495165620528,\n",
       " 0.12949354932618545,\n",
       " 0.1294722300688761,\n",
       " 0.1294284943093164,\n",
       " 0.12937947279209552,\n",
       " 0.12926638865276463,\n",
       " 0.12915410953591672,\n",
       " 0.12911409043081765,\n",
       " 0.12910895694329533,\n",
       " 0.12909824639796239,\n",
       " 0.12903181990591608,\n",
       " 0.12900530389669645,\n",
       " 0.1290033711711331,\n",
       " 0.1288159014337431,\n",
       " 0.1287696343330416,\n",
       " 0.12876636097342214,\n",
       " 0.12871975598044405)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49411764705882355"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(aspects).intersection(set(entities)))/len(set(aspects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(f\"../data/{domain}.test.txt\", \"r\").read().splitlines()\n",
    "with open(\"/root/autodl-tmp/out/bert_base/laptop-rest/predict.txt\", \"r\") as f:\n",
    "    pre_aspects = []\n",
    "    for i, line in enumerate(f):\n",
    "        _, pre, labels = line.split(\"***\")\n",
    "        tokens = data[i].split(\"***\")[0]\n",
    "        pres = pre.split()\n",
    "        labels = labels.strip().split()\n",
    "        pre_aspect = []\n",
    "        for idx, label in enumerate(pres):\n",
    "            if label != 'O':\n",
    "                pre_aspect.append(tokens[idx])\n",
    "                continue\n",
    "            if pre_aspect:\n",
    "                pre_aspects.append(' '.join(pre_aspect))\n",
    "                pre_aspect.clear()\n",
    "len(set(pre_aspects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(pre_aspects).intersection(set(entities)))/len(set(pre_aspects))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "514bf37689fab66dd9e64044d554682626457c62a79ef980ffeb215c6b270f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
