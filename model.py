from dataclasses import dataclass

import torch
import torch.nn as nn
from torch import Tensor
from pytorch_revgrad import RevGrad
from pytorch_transformers import BertModel, BertPreTrainedModel
from transformers.modeling_outputs import TokenClassifierOutput
from torch.nn import CrossEntropyLoss
import torch.nn.functional as f


class SoftEntropy(nn.Module):

    def __init__(self):
        super(SoftEntropy, self).__init__()
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, inputs, targets):
        log_probs = self.logsoftmax(inputs)
        loss = (-f.softmax(targets, dim=1).detach() * log_probs).mean(0).sum()
        return loss


__all__ = ['BertForTokenClassification', 'MMTModel', 'DomainModel']


class DomainModel(nn.Module):

    def __init__(self, dim: int, adv: float = 1.0):
        super(DomainModel, self).__init__()
        out_dim = 100
        self.project = nn.Sequential(nn.Tanh(), nn.Linear(dim, out_dim), nn.Linear(out_dim, 1),
                                     nn.Softmax(dim=1))
        self.full_layer = nn.Linear(dim, 1)
        self.grl = RevGrad(adv)

    def forward(self, inputs):
        temp = self.grl(inputs)
        alphas = self.project(temp)
        x = torch.sum(temp * alphas, dim=1)
        final_out = self.full_layer(x)
        return final_out.squeeze(1), alphas


class BertForTokenClassification(BertPreTrainedModel):

    def __init__(self, config):
        super(BertForTokenClassification, self).__init__(config)
        self.num_labels = config.num_labels
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
        self.loss_fct = CrossEntropyLoss(ignore_index=-1)

    def forward(self,
                input_ids: Tensor,
                token_type_ids: Tensor = None,
                attention_mask: Tensor = None,
                **kwargs):
        labels = kwargs.get("gold_labels", None)
        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
        sequence_output = outputs[0]

        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)  # attention_mask size (batch, seq_len)
        loss = None
        if labels is not None:
            # Only keep active parts of the loss
            if attention_mask is not None:
                active_loss = attention_mask.view(-1) == 1
                active_logits = logits.view(-1, self.num_labels)[active_loss]
                active_labels = labels.view(-1)[active_loss]
                loss = self.loss_fct(active_logits, active_labels)
            else:
                loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        return TokenClassifierOutput(logits=logits, loss=loss, hidden_states=outputs)

    def post_operation(self, *args, **kwargs):
        pass


class MMTModel(nn.Module):

    def __init__(self,
                 model_1: BertForTokenClassification,
                 model_ema_1: BertForTokenClassification,
                 model_2: BertForTokenClassification,
                 model_ema_2: BertForTokenClassification,
                 dom_model: DomainModel,
                 alpha: float = 0.999,
                 ce_soft_weight: float = 0.5,
                 domain_loss_weight: float = 0.1,
                 domain_loss=nn.BCEWithLogitsLoss(),
                 ce_loss=nn.CrossEntropyLoss(ignore_index=-1),
                 ce_soft_loss=SoftEntropy()):
        super(MMTModel, self).__init__()
        self.model_1 = model_1
        self.model_ema_1 = model_ema_1
        self.model_2 = model_2
        self.model_ema_2 = model_ema_2
        self.dom_model = dom_model
        self.alpha = alpha
        self.ce_soft_weight = ce_soft_weight
        self.domain_loss_weight = domain_loss_weight
        self.domain_loss = domain_loss
        self.ce_loss = ce_loss
        self.ce_soft_loss = ce_soft_loss

    def forward(self,
                input_ids: Tensor,
                token_type_ids: Tensor = None,
                attention_mask: Tensor = None,
                **kwargs):
        gold_labels = kwargs.get("gold_labels")
        hard_labels = kwargs.get("hard_labels")
        domains = kwargs.get("domains")
        input_ids_1, input_ids_2 = torch.chunk(input_ids, 2)
        token_type_ids_1, token_type_ids_2 = torch.chunk(token_type_ids, 2)
        attention_mask_1, attention_mask_2 = torch.chunk(attention_mask, 2)
        # for source domain, supervised with gold labels
        # for target domain, supervised with hard labels generated by double propagation
        labels = torch.where(domains, gold_labels, hard_labels)
        labels_1, labels_2 = torch.chunk(labels, 2)
        outputs1: TokenClassifierOutput = self.model_1(input_ids_1,
                                                       token_type_ids_1,
                                                       attention_mask_1,
                                                       gold_labels=labels_1)
        outputs1_ema: TokenClassifierOutput = self.model_ema_1(input_ids_1, token_type_ids_1,
                                                               attention_mask_1)
        outputs2: TokenClassifierOutput = self.model_2(input_ids_2,
                                                       token_type_ids_2,
                                                       attention_mask_2,
                                                       gold_labels=labels_2)
        outputs2_ema: TokenClassifierOutput = self.model_ema_2(input_ids_2, token_type_ids_2,
                                                               attention_mask_2)
        loss_ce_1 = outputs1.loss
        loss_ce_2 = outputs2.loss
        domains_1, domains_2 = torch.chunk(torch.as_tensor(domains.squeeze(-1), dtype=float), 2)
        domain_pre, _ = self.dom_model(outputs1.hidden_states[0])
        d_loss_1 = self.domain_loss(domain_pre, domains_1)
        domain_pre, _ = self.dom_model(outputs2.hidden_states[0])
        d_loss_2 = self.domain_loss(domain_pre, domains_2)
        d_loss = d_loss_1 + d_loss_2
        loss_ce_soft = self.ce_soft_loss(outputs1.logits, outputs2_ema.logits) + self.ce_soft_loss(
            outputs2.logits, outputs1_ema.logits)

        loss = (loss_ce_1 + loss_ce_2) * (1 - self.ce_soft_weight) + \
             loss_ce_soft * self.ce_soft_weight + d_loss * self.domain_loss_weight
        return TokenClassifierOutput(logits=torch.cat([outputs1.logits, outputs2.logits]),
                                     loss=loss)

    def post_operation(self, *args, **kwargs):
        global_step = kwargs.get("global_step")
        self.__update_ema_variables(self.model_1, self.model_ema_1, self.alpha, global_step)
        self.__update_ema_variables(self.model_2, self.model_ema_2, self.alpha, global_step)

    def __update_ema_variables(self, model, ema_model, alpha, global_step):
        alpha = min(1 - 1 / (global_step + 1), alpha)
        for ema_param, param in zip(ema_model.parameters(), model.parameters()):
            ema_param.data.mul_(alpha).add_(1 - alpha, param.data)
